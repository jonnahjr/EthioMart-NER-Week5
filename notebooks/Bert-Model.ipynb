{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9515631,"sourceType":"datasetVersion","datasetId":5792923},{"sourceId":9516028,"sourceType":"datasetVersion","datasetId":5793238},{"sourceId":9522276,"sourceType":"datasetVersion","datasetId":5797879}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EthioMart - Named Entity Recognition for Amharic E-commerce Data\n\n## Overview\n\n**EthioMart**, a growing hub for Telegram-based e-commerce in Ethiopia, aims to consolidate multiple independent e-commerce channels into a single centralized platform. With the increasing use of Telegram for business transactions, customers and vendors are currently spread across various channels, leading to challenges in product discovery, communication, and order management.\n\nThis project focuses on building an Amharic Named Entity Recognition (NER) system to extract important business entitiessuch as product names, prices, and locationsfrom the messages shared in these Telegram channels. The extracted data will be used to populate EthioMart's centralized database, providing a seamless and organized shopping experience for customers and a unified platform for vendors.\n\n## Key Objectives\n\n1. Real-time Data Extraction: Fetch data from various Ethiopian Telegram e-commerce channels.\n1. Fine-tuning Large Language Models (LLMs): Adapt existing LLMs to accurately extract business entities like product names, prices, and locations from Amharic text.","metadata":{}},{"cell_type":"markdown","source":"## Install Necessary Packages","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets seqeval","metadata":{"_uuid":"6080e595-0b96-4e70-9afa-55efb791a5d9","_cell_guid":"3ddeabd4-827a-466c-aac5-0ee7a522d96e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:32:58.489766Z","iopub.execute_input":"2024-10-01T18:32:58.490517Z","iopub.status.idle":"2024-10-01T18:33:15.244020Z","shell.execute_reply.started":"2024-10-01T18:32:58.490470Z","shell.execute_reply":"2024-10-01T18:33:15.243052Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.0)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=b348e3fd2eef49f44a01ea3a989af36cb4b0505fc1169df0a2b18bdd80e96673\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\nfrom transformers import DataCollatorForTokenClassification\nfrom seqeval.metrics import accuracy_score, classification_report\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:34:40.815142Z","iopub.execute_input":"2024-10-01T18:34:40.815561Z","iopub.status.idle":"2024-10-01T18:35:07.665044Z","shell.execute_reply.started":"2024-10-01T18:34:40.815525Z","shell.execute_reply":"2024-10-01T18:35:07.664256Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Load the Dataset","metadata":{}},{"cell_type":"code","source":"def load_conll_dataset(file_path):\n    sentences, labels = [], []\n    sentence, label = [], []\n    \n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        for line in file:\n            if line.strip():  # If the line is not empty\n                parts = line.strip().split()\n                if len(parts) == 2:  # Ensure the line has exactly two components\n                    token, tag = parts\n                    sentence.append(token)\n                    label.append(tag)\n                else:\n                    print(f\"Skipping malformed line: {line.strip()}\")\n            else:\n                if sentence:  # Append only if the sentence is not empty\n                    sentences.append(sentence)\n                    labels.append(label)\n                sentence, label = [], []\n    \n    if sentence:  # Append any remaining sentence\n        sentences.append(sentence)\n        labels.append(label)\n    \n    return pd.DataFrame({\"tokens\": sentences, \"ner_tags\": labels})\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:07.667041Z","iopub.execute_input":"2024-10-01T18:35:07.667911Z","iopub.status.idle":"2024-10-01T18:35:07.675972Z","shell.execute_reply.started":"2024-10-01T18:35:07.667864Z","shell.execute_reply":"2024-10-01T18:35:07.675083Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Split the dataset","metadata":{}},{"cell_type":"code","source":"# Function to split the CoNLL dataset into training and validation sets\ndef split_conll_dataset(conll_df, train_ratio=0.8):\n    # Split the dataset into train and validation sets\n    train_df, val_df = train_test_split(conll_df, train_size=train_ratio, random_state=42, shuffle=True)\n\n    return train_df, val_df\n\n# Example usage\nfile_path = \"/kaggle/input/reduced-data/labeled_ner_data.txt\"\nconll_df = load_conll_dataset(file_path)  # Load the dataset\ntrain_dataset, val_dataset = split_conll_dataset(conll_df)  # Split the dataset\n\n# Check the sizes of the resulting datasets\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:07.677181Z","iopub.execute_input":"2024-10-01T18:35:07.677585Z","iopub.status.idle":"2024-10-01T18:35:07.832026Z","shell.execute_reply.started":"2024-10-01T18:35:07.677539Z","shell.execute_reply":"2024-10-01T18:35:07.831115Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training set size: 1072\nValidation set size: 268\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Label Mapping","metadata":{}},{"cell_type":"code","source":"# Define label mapping\nlabel_to_id = {\n    \"O\": 0,  # Outside of entity\n    \"B-Product\": 1,  # Beginning of a Product entity\n    \"I-Product\": 2,  # Inside of a Product entity\n    \"B-PRICE\": 3,  # Beginning of a Price entity\n    \"I-PRICE\": 4,  # Inside of a Price entity\n    \"B-LOC\": 5,  # Beginning of a Location entity\n    \"I-LOC\": 6   # Inside of a Location entity\n}\n\n# Reverse mapping for predictions\nid_to_label = {v: k for k, v in label_to_id.items()}","metadata":{"_uuid":"e2ef1337-2be6-4097-8e84-7c520b4c5c49","_cell_guid":"cd0ca3db-02b3-4f8c-a6ff-bd1d220e7a1b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:35:07.833910Z","iopub.execute_input":"2024-10-01T18:35:07.834238Z","iopub.status.idle":"2024-10-01T18:35:07.841108Z","shell.execute_reply.started":"2024-10-01T18:35:07.834187Z","shell.execute_reply":"2024-10-01T18:35:07.840175Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize and Align Labels","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(examples, tokenizer, label_all_tokens=True):\n    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n    \n    labels = []\n    for i, label in enumerate(examples['ner_tags']):\n        # Replace any '0' (zero) with 'O' (uppercase letter O) and 'o' (lowercase) with 'O'\n        label = ['O' if l in ['0', 'o'] else l for l in label]\n        \n        # Convert string labels to integers using label_to_id mapping\n        label = [label_to_id[l] for l in label]  # Mapping the string NER tags to integers\n        \n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)  # Padding token\n            elif word_idx != previous_word_idx:  # First token of a word\n                label_ids.append(label[word_idx])\n            else:  # Non-first token of a word\n                label_ids.append(-100 if not label_all_tokens else label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    \n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"_uuid":"50bae40b-bc81-457b-aeac-06328fcec5ad","_cell_guid":"dbb0470a-7c23-4e4b-a947-9aefe0562785","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:35:07.842422Z","iopub.execute_input":"2024-10-01T18:35:07.842764Z","iopub.status.idle":"2024-10-01T18:35:07.851715Z","shell.execute_reply.started":"2024-10-01T18:35:07.842733Z","shell.execute_reply":"2024-10-01T18:35:07.850938Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Train and Evaluate Model","metadata":{}},{"cell_type":"code","source":"# Function to fine-tune the model\ndef train_and_evaluate_model(model_name, train_dataset, val_dataset, label_list, batch_size=16, epochs=15):\n    print(f\"Training model: {model_name}\")\n    \n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n\n    # Tokenize dataset\n    # Passing tokenizer inside lambda function\n    training_dataset = train_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=True)\n    evaluation_dataset = val_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=True)\n\n    # Data collator\n    data_collator = DataCollatorForTokenClassification(tokenizer)\n    \n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=f\"./results_{model_name}\",\n        evaluation_strategy=\"epoch\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=epochs,\n        weight_decay=0.01,\n        logging_dir=f\"./logs_{model_name}\",\n        logging_steps=50\n    )\n\n    # Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=training_dataset,\n        eval_dataset=evaluation_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate the model\n    eval_results = trainer.evaluate()\n    print(f\"Evaluation results for {model_name}:\", eval_results)\n    return eval_results","metadata":{"_uuid":"7912b49f-ef9f-4381-a315-b2570cd671f9","_cell_guid":"2646fa32-0c14-4858-a6fa-52290d4cda5d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:35:07.852838Z","iopub.execute_input":"2024-10-01T18:35:07.853602Z","iopub.status.idle":"2024-10-01T18:35:07.867794Z","shell.execute_reply.started":"2024-10-01T18:35:07.853571Z","shell.execute_reply":"2024-10-01T18:35:07.867012Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Compute Metrics","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    # Retrieve predictions and true labels\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    # Convert numeric labels back to their string names using id_to_label mapping\n    true_labels = [[id_to_label[l] for l in label_row if l != -100] for label_row in labels]\n    true_preds = [[id_to_label[p] for (p, l) in zip(pred_row, label_row) if l != -100] for pred_row, label_row in zip(preds, labels)]\n    \n    # Use seqeval to evaluate the performance\n    report = classification_report(true_labels, true_preds)\n    accuracy = accuracy_score(true_labels, true_preds)\n    \n    return {\"accuracy\": accuracy, \"report\": report}","metadata":{"_uuid":"03786df1-ee66-4073-904b-0eacb5c8a096","_cell_guid":"f514d309-e38d-4136-a88c-3caf8fa00208","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:35:07.868838Z","iopub.execute_input":"2024-10-01T18:35:07.869245Z","iopub.status.idle":"2024-10-01T18:35:07.881305Z","shell.execute_reply.started":"2024-10-01T18:35:07.869187Z","shell.execute_reply":"2024-10-01T18:35:07.880542Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Compare Models","metadata":{}},{"cell_type":"code","source":"# Function to compare models\ndef compare_models(models, dataset, label_list):\n    results = {}\n    for model_name in models:\n        eval_result = train_and_evaluate_model(model_name, dataset, label_list)\n        results[model_name] = eval_result\n    return results","metadata":{"_uuid":"dcbc82f0-e0c7-4e7b-9c7e-653a72d7f3dd","_cell_guid":"4e8d9dfa-65cd-4bd6-b214-63b62e378ee0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:35:07.882496Z","iopub.execute_input":"2024-10-01T18:35:07.882919Z","iopub.status.idle":"2024-10-01T18:35:07.894454Z","shell.execute_reply.started":"2024-10-01T18:35:07.882877Z","shell.execute_reply":"2024-10-01T18:35:07.893707Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Load the labeled CoNLL dataset\n#conll_df = load_conll_dataset(\"/kaggle/input/collection-ner/NER_Collection_data.txt\")\n#dataset = Dataset.from_pandas(conll_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:07.895597Z","iopub.execute_input":"2024-10-01T18:35:07.896010Z","iopub.status.idle":"2024-10-01T18:35:07.904359Z","shell.execute_reply.started":"2024-10-01T18:35:07.895969Z","shell.execute_reply":"2024-10-01T18:35:07.903627Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Count Labels in the Dataset","metadata":{}},{"cell_type":"code","source":"# Function to count each label in the dataset\ndef count_labels(dataset):\n    all_labels = [label for labels in dataset['ner_tags'] for label in labels]\n    label_counts = Counter(all_labels)\n    \n    # Print the counts for each label\n    for label, count in label_counts.items():\n        print(f\"Label: {label}, Count: {count}\")\n    \n    return label_counts","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:07.907152Z","iopub.execute_input":"2024-10-01T18:35:07.907529Z","iopub.status.idle":"2024-10-01T18:35:07.915090Z","shell.execute_reply.started":"2024-10-01T18:35:07.907498Z","shell.execute_reply":"2024-10-01T18:35:07.914383Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_label_counts = count_labels(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:44.919067Z","iopub.execute_input":"2024-10-01T18:35:44.919483Z","iopub.status.idle":"2024-10-01T18:35:44.932612Z","shell.execute_reply.started":"2024-10-01T18:35:44.919446Z","shell.execute_reply":"2024-10-01T18:35:44.931608Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Label: O, Count: 26629\nLabel: B-Product, Count: 1096\nLabel: I-Product, Count: 168\nLabel: B-LOC, Count: 3178\nLabel: I-LOC, Count: 28566\nLabel: B-Price, Count: 263\nLabel: I-Price, Count: 263\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluation_label_counts = count_labels(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:46.102069Z","iopub.execute_input":"2024-10-01T18:35:46.102437Z","iopub.status.idle":"2024-10-01T18:35:46.108876Z","shell.execute_reply.started":"2024-10-01T18:35:46.102401Z","shell.execute_reply":"2024-10-01T18:35:46.108013Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Label: B-Product, Count: 295\nLabel: O, Count: 7119\nLabel: B-LOC, Count: 829\nLabel: I-LOC, Count: 7388\nLabel: B-Price, Count: 67\nLabel: I-Price, Count: 67\nLabel: I-Product, Count: 44\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Map labels to correct labels","metadata":{}},{"cell_type":"code","source":"# Function to map incorrect labels to correct labels\ndef map_labels(dataset):\n    # Define the mapping from incorrect to correct labels\n    label_mapping = {\n        'B-PROD': 'B-Product',   # Map 'B-PROD' to 'B-Product'\n        'B-PRODUCT': 'B-Product', # Map 'B-PRODUCT' to 'B-Product'\n        'I-PRODUCT': 'I-Product', # Map 'I-PRODUCT' to 'I-Product'\n        'B-Price': 'B-PRICE',    # Map 'B-Price' to 'B-PRICE'\n        'I-Price': 'I-PRICE',    # Map 'I-Price' to 'I-PRICE'\n        'IO': 'O'                # Map 'IO' to 'O'\n    }\n    \n    # Replace the incorrect labels with the correct ones\n    dataset['ner_tags'] = dataset['ner_tags'].apply(\n        lambda tags: [label_mapping.get(tag, tag) for tag in tags]\n    )\n    \n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:51.278591Z","iopub.execute_input":"2024-10-01T18:35:51.279253Z","iopub.status.idle":"2024-10-01T18:35:51.285482Z","shell.execute_reply.started":"2024-10-01T18:35:51.279213Z","shell.execute_reply":"2024-10-01T18:35:51.284443Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Example usage:\ntrain_df = map_labels(train_dataset)\ntrain_dataset = Dataset.from_pandas(train_df)\n\n# Verify the label counts after remapping\nlabel_counts = count_labels(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:52.750152Z","iopub.execute_input":"2024-10-01T18:35:52.750545Z","iopub.status.idle":"2024-10-01T18:35:52.830145Z","shell.execute_reply.started":"2024-10-01T18:35:52.750508Z","shell.execute_reply":"2024-10-01T18:35:52.829227Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Label: O, Count: 26629\nLabel: B-Product, Count: 1096\nLabel: I-Product, Count: 168\nLabel: B-LOC, Count: 3178\nLabel: I-LOC, Count: 28566\nLabel: B-PRICE, Count: 263\nLabel: I-PRICE, Count: 263\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example usage:\nval_df = map_labels(val_dataset)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Verify the label counts after remapping\nlabel_counts = count_labels(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:35:54.224060Z","iopub.execute_input":"2024-10-01T18:35:54.224985Z","iopub.status.idle":"2024-10-01T18:35:54.249783Z","shell.execute_reply.started":"2024-10-01T18:35:54.224935Z","shell.execute_reply":"2024-10-01T18:35:54.248953Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Label: B-Product, Count: 295\nLabel: O, Count: 7119\nLabel: B-LOC, Count: 829\nLabel: I-LOC, Count: 7388\nLabel: B-PRICE, Count: 67\nLabel: I-PRICE, Count: 67\nLabel: I-Product, Count: 44\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example usage:\nconll_df = map_labels(conll_df)\n\n# Verify the label counts after remapping\nlabel_counts = count_labels(conll_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:36:06.854259Z","iopub.execute_input":"2024-10-01T18:36:06.854652Z","iopub.status.idle":"2024-10-01T18:36:06.878033Z","shell.execute_reply.started":"2024-10-01T18:36:06.854617Z","shell.execute_reply":"2024-10-01T18:36:06.877104Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Label: O, Count: 33748\nLabel: B-Product, Count: 1391\nLabel: I-Product, Count: 212\nLabel: B-LOC, Count: 4007\nLabel: I-LOC, Count: 35954\nLabel: B-PRICE, Count: 330\nLabel: I-PRICE, Count: 330\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n# Function to save the dataset to storage\ndef save_dataset(dataset, file_path):\n    \n    Save the modified dataset to a specified file path in CSV format.\n    \n    Args:\n        dataset (pd.DataFrame): The DataFrame containing the dataset.\n        file_path (str): The file path where the dataset will be saved.\n    \n    dataset.to_csv(file_path, index=False)\n    print(f\"Dataset saved to {file_path}\")\n\n# Save the mapped dataset to a CSV file\nsave_dataset(conll_df, \"preprocessed_conll_data.txt\")\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:18:58.583295Z","iopub.execute_input":"2024-10-01T18:18:58.583692Z","iopub.status.idle":"2024-10-01T18:18:58.591301Z","shell.execute_reply.started":"2024-10-01T18:18:58.583649Z","shell.execute_reply":"2024-10-01T18:18:58.590432Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'\\n# Function to save the dataset to storage\\ndef save_dataset(dataset, file_path):\\n    \\n    Save the modified dataset to a specified file path in CSV format.\\n    \\n    Args:\\n        dataset (pd.DataFrame): The DataFrame containing the dataset.\\n        file_path (str): The file path where the dataset will be saved.\\n    \\n    dataset.to_csv(file_path, index=False)\\n    print(f\"Dataset saved to {file_path}\")\\n\\n# Save the mapped dataset to a CSV file\\nsave_dataset(conll_df, \"preprocessed_conll_data.txt\")\\n\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### List Models and Labels","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# List of entity labels \nlabel_list = ['O', 'B-Product', 'I-Product', 'B-PRICE', 'I-PRICE', 'B-LOC', 'I-LOC']\n\n# Define models for comparison\nmodels = [\n    \"xlm-roberta-base\",  \n    \"bert-base-multilingual-cased\",  \n    \"distilbert-base-multilingual-cased\"  \n]\n\n# Compare models\nresults = compare_models(models, dataset, label_list)\n\n\"\"\"","metadata":{"_uuid":"7633621f-29d0-4f50-b964-72f8b20aa6c4","_cell_guid":"ca615b81-0b23-47fb-8396-b0d615baaae0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:18:58.592305Z","iopub.execute_input":"2024-10-01T18:18:58.592581Z","iopub.status.idle":"2024-10-01T18:18:58.601184Z","shell.execute_reply.started":"2024-10-01T18:18:58.592551Z","shell.execute_reply":"2024-10-01T18:18:58.600312Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'\\n# List of entity labels \\nlabel_list = [\\'O\\', \\'B-Product\\', \\'I-Product\\', \\'B-PRICE\\', \\'I-PRICE\\', \\'B-LOC\\', \\'I-LOC\\']\\n\\n# Define models for comparison\\nmodels = [\\n    \"xlm-roberta-base\",  \\n    \"bert-base-multilingual-cased\",  \\n    \"distilbert-base-multilingual-cased\"  \\n]\\n\\n# Compare models\\nresults = compare_models(models, dataset, label_list)\\n\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Print Comparison Results","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Print out comparison results\nfor model_name, result in results.items():\n    print(f\"Model: {model_name}\")\n    print(f\"Accuracy: {result['eval_accuracy']}\")\n    print(result['eval_report'])\n\n\"\"\"","metadata":{"_uuid":"6947d727-0c47-43d1-b321-fdda05a9cbaf","_cell_guid":"2c2fc99d-15e0-4faa-b1d6-bdec16fb6896","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:18:58.602372Z","iopub.execute_input":"2024-10-01T18:18:58.603023Z","iopub.status.idle":"2024-10-01T18:18:58.612136Z","shell.execute_reply.started":"2024-10-01T18:18:58.602962Z","shell.execute_reply":"2024-10-01T18:18:58.611292Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'\\n# Print out comparison results\\nfor model_name, result in results.items():\\n    print(f\"Model: {model_name}\")\\n    print(f\"Accuracy: {result[\\'eval_accuracy\\']}\")\\n    print(result[\\'eval_report\\'])\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"#API = c912e406b425b51cb31ae3db26397612b381918d","metadata":{"_uuid":"2e2552c0-f9ac-4982-a606-d99f209c07f9","_cell_guid":"aaea5b19-240d-468d-94e5-ee289e21ad73","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-01T18:18:58.613137Z","iopub.execute_input":"2024-10-01T18:18:58.613424Z","iopub.status.idle":"2024-10-01T18:18:58.623936Z","shell.execute_reply.started":"2024-10-01T18:18:58.613393Z","shell.execute_reply":"2024-10-01T18:18:58.623055Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Fine-tune a Single Model at a Time","metadata":{}},{"cell_type":"code","source":"# Function to train and evaluate one model\ndef run_single_model(model_name, train_dataset, val_dataset, label_list):\n    # Train and evaluate the model\n    eval_result = train_and_evaluate_model(model_name, train_dataset, val_dataset, label_list)\n    \n    # Print the evaluation result for the model\n    print(f\"Model: {model_name}\")\n    print(f\"Accuracy: {eval_result['eval_accuracy']}\")\n    print(eval_result['eval_report'])\n    \n    return eval_result\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:36:20.434248Z","iopub.execute_input":"2024-10-01T18:36:20.434652Z","iopub.status.idle":"2024-10-01T18:36:20.440439Z","shell.execute_reply.started":"2024-10-01T18:36:20.434617Z","shell.execute_reply":"2024-10-01T18:36:20.439456Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# List of entity labels \nlabel_list = ['O', 'B-Product', 'I-Product', 'B-PRICE', 'I-PRICE', 'B-LOC', 'I-LOC']\n\n# Define the model to run\nmodel_name = \"bert-base-multilingual-cased\"\n\n# Run and evaluate the model\neval_result = run_single_model(model_name, train_dataset, val_dataset, label_list)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T18:36:24.438720Z","iopub.execute_input":"2024-10-01T18:36:24.439112Z","iopub.status.idle":"2024-10-01T18:42:12.103949Z","shell.execute_reply.started":"2024-10-01T18:36:24.439074Z","shell.execute_reply":"2024-10-01T18:42:12.102837Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Training model: bert-base-multilingual-cased\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ea9d7b863254d3b97c9fed0cdd692b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b0ff78dd56d420792a06289ec92ed5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53d0c19f3bde4400a7de826920f24a9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35bc0cf35dc04f4b9107ece0f49574cc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d499a882ad2545ca93952cf5765466bf"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1072 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a358fb02ed94394b0b652c247e18437"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"064e16cde76348bdbb3bd2c9d9c2659c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241001_183651-gii1mysw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tedoaba-t/huggingface/runs/gii1mysw' target=\"_blank\">./results_bert-base-multilingual-cased</a></strong> to <a href='https://wandb.ai/tedoaba-t/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tedoaba-t/huggingface' target=\"_blank\">https://wandb.ai/tedoaba-t/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tedoaba-t/huggingface/runs/gii1mysw' target=\"_blank\">https://wandb.ai/tedoaba-t/huggingface/runs/gii1mysw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1005' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1005/1005 05:14, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Report</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.595700</td>\n      <td>0.351850</td>\n      <td>0.886954</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.07      0.02      0.03       829\n       PRICE       0.00      0.00      0.00        74\n     Product       0.00      0.00      0.00       295\n\n   micro avg       0.07      0.02      0.03      1198\n   macro avg       0.02      0.01      0.01      1198\nweighted avg       0.05      0.02      0.02      1198\n</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.364500</td>\n      <td>0.283544</td>\n      <td>0.914671</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.30      0.09      0.14       829\n       PRICE       0.81      0.74      0.77        74\n     Product       0.98      0.38      0.54       295\n\n   micro avg       0.56      0.20      0.30      1198\n   macro avg       0.70      0.40      0.49      1198\nweighted avg       0.50      0.20      0.28      1198\n</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.286800</td>\n      <td>0.269823</td>\n      <td>0.919875</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.31      0.29      0.30       829\n       PRICE       0.90      0.93      0.91        74\n     Product       0.97      0.38      0.54       295\n\n   micro avg       0.43      0.35      0.39      1198\n   macro avg       0.73      0.53      0.59      1198\nweighted avg       0.51      0.35      0.40      1198\n</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.261200</td>\n      <td>0.237010</td>\n      <td>0.922689</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.24      0.18      0.20       829\n       PRICE       0.92      0.89      0.90        74\n     Product       0.81      0.39      0.53       295\n\n   micro avg       0.40      0.27      0.33      1198\n   macro avg       0.65      0.49      0.55      1198\nweighted avg       0.42      0.27      0.33      1198\n</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.237900</td>\n      <td>0.176034</td>\n      <td>0.929326</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.44      0.35      0.39       829\n       PRICE       0.91      0.95      0.93        74\n     Product       0.70      0.47      0.56       295\n\n   micro avg       0.53      0.42      0.47      1198\n   macro avg       0.68      0.59      0.63      1198\nweighted avg       0.53      0.42      0.47      1198\n</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.183300</td>\n      <td>0.142353</td>\n      <td>0.954123</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.66      0.67      0.67       829\n       PRICE       0.94      0.99      0.96        74\n     Product       0.81      0.44      0.57       295\n\n   micro avg       0.71      0.63      0.67      1198\n   macro avg       0.80      0.70      0.73      1198\nweighted avg       0.72      0.63      0.66      1198\n</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.159800</td>\n      <td>0.130841</td>\n      <td>0.957946</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.78      0.70      0.73       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.72      0.46      0.56       295\n\n   micro avg       0.78      0.66      0.71      1198\n   macro avg       0.82      0.71      0.75      1198\nweighted avg       0.77      0.66      0.71      1198\n</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.157200</td>\n      <td>0.119196</td>\n      <td>0.959964</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.80      0.71      0.75       829\n       PRICE       0.96      0.97      0.97        74\n     Product       0.79      0.49      0.60       295\n\n   micro avg       0.81      0.67      0.73      1198\n   macro avg       0.85      0.72      0.77      1198\nweighted avg       0.81      0.67      0.73      1198\n</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.126900</td>\n      <td>0.108665</td>\n      <td>0.967026</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.86      0.80      0.83       829\n       PRICE       0.95      0.97      0.96        74\n     Product       0.92      0.50      0.65       295\n\n   micro avg       0.88      0.73      0.80      1198\n   macro avg       0.91      0.76      0.81      1198\nweighted avg       0.88      0.73      0.79      1198\n</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.122800</td>\n      <td>0.096416</td>\n      <td>0.973132</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.85      0.81      0.83       829\n       PRICE       0.96      0.99      0.97        74\n     Product       0.90      0.50      0.64       295\n\n   micro avg       0.87      0.74      0.80      1198\n   macro avg       0.90      0.76      0.81      1198\nweighted avg       0.87      0.74      0.79      1198\n</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.115800</td>\n      <td>0.091641</td>\n      <td>0.973876</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.88      0.85      0.87       829\n       PRICE       0.96      0.99      0.97        74\n     Product       0.95      0.50      0.66       295\n\n   micro avg       0.90      0.77      0.83      1198\n   macro avg       0.93      0.78      0.83      1198\nweighted avg       0.91      0.77      0.82      1198\n</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.101300</td>\n      <td>0.088148</td>\n      <td>0.977752</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.93      0.88      0.90       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.98      0.51      0.67       295\n\n   micro avg       0.94      0.79      0.86      1198\n   macro avg       0.95      0.79      0.85      1198\nweighted avg       0.94      0.79      0.85      1198\n</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.093500</td>\n      <td>0.086599</td>\n      <td>0.976477</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.93      0.89      0.91       829\n       PRICE       0.96      0.99      0.97        74\n     Product       0.98      0.51      0.67       295\n\n   micro avg       0.94      0.80      0.86      1198\n   macro avg       0.96      0.79      0.85      1198\nweighted avg       0.94      0.80      0.85      1198\n</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.093300</td>\n      <td>0.085443</td>\n      <td>0.977274</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.94      0.90      0.91       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.98      0.50      0.66       295\n\n   micro avg       0.94      0.80      0.87      1198\n   macro avg       0.95      0.79      0.85      1198\nweighted avg       0.95      0.80      0.86      1198\n</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.087000</td>\n      <td>0.083998</td>\n      <td>0.978070</td>\n      <td>              precision    recall  f1-score   support\n\n         LOC       0.94      0.89      0.91       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.97      0.51      0.67       295\n\n   micro avg       0.94      0.80      0.87      1198\n   macro avg       0.95      0.79      0.85      1198\nweighted avg       0.95      0.80      0.85      1198\n</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.07      0.02      0.03       829\n       PRICE       0.00      0.00      0.00        74\n     Product       0.00      0.00      0.00       295\n\n   micro avg       0.07      0.02      0.03      1198\n   macro avg       0.02      0.01      0.01      1198\nweighted avg       0.05      0.02      0.02      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.30      0.09      0.14       829\n       PRICE       0.81      0.74      0.77        74\n     Product       0.98      0.38      0.54       295\n\n   micro avg       0.56      0.20      0.30      1198\n   macro avg       0.70      0.40      0.49      1198\nweighted avg       0.50      0.20      0.28      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.31      0.29      0.30       829\n       PRICE       0.90      0.93      0.91        74\n     Product       0.97      0.38      0.54       295\n\n   micro avg       0.43      0.35      0.39      1198\n   macro avg       0.73      0.53      0.59      1198\nweighted avg       0.51      0.35      0.40      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.24      0.18      0.20       829\n       PRICE       0.92      0.89      0.90        74\n     Product       0.81      0.39      0.53       295\n\n   micro avg       0.40      0.27      0.33      1198\n   macro avg       0.65      0.49      0.55      1198\nweighted avg       0.42      0.27      0.33      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.44      0.35      0.39       829\n       PRICE       0.91      0.95      0.93        74\n     Product       0.70      0.47      0.56       295\n\n   micro avg       0.53      0.42      0.47      1198\n   macro avg       0.68      0.59      0.63      1198\nweighted avg       0.53      0.42      0.47      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.66      0.67      0.67       829\n       PRICE       0.94      0.99      0.96        74\n     Product       0.81      0.44      0.57       295\n\n   micro avg       0.71      0.63      0.67      1198\n   macro avg       0.80      0.70      0.73      1198\nweighted avg       0.72      0.63      0.66      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.78      0.70      0.73       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.72      0.46      0.56       295\n\n   micro avg       0.78      0.66      0.71      1198\n   macro avg       0.82      0.71      0.75      1198\nweighted avg       0.77      0.66      0.71      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.80      0.71      0.75       829\n       PRICE       0.96      0.97      0.97        74\n     Product       0.79      0.49      0.60       295\n\n   micro avg       0.81      0.67      0.73      1198\n   macro avg       0.85      0.72      0.77      1198\nweighted avg       0.81      0.67      0.73      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.86      0.80      0.83       829\n       PRICE       0.95      0.97      0.96        74\n     Product       0.92      0.50      0.65       295\n\n   micro avg       0.88      0.73      0.80      1198\n   macro avg       0.91      0.76      0.81      1198\nweighted avg       0.88      0.73      0.79      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.85      0.81      0.83       829\n       PRICE       0.96      0.99      0.97        74\n     Product       0.90      0.50      0.64       295\n\n   micro avg       0.87      0.74      0.80      1198\n   macro avg       0.90      0.76      0.81      1198\nweighted avg       0.87      0.74      0.79      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.88      0.85      0.87       829\n       PRICE       0.96      0.99      0.97        74\n     Product       0.95      0.50      0.66       295\n\n   micro avg       0.90      0.77      0.83      1198\n   macro avg       0.93      0.78      0.83      1198\nweighted avg       0.91      0.77      0.82      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.93      0.88      0.90       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.98      0.51      0.67       295\n\n   micro avg       0.94      0.79      0.86      1198\n   macro avg       0.95      0.79      0.85      1198\nweighted avg       0.94      0.79      0.85      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.93      0.89      0.91       829\n       PRICE       0.96      0.99      0.97        74\n     Product       0.98      0.51      0.67       295\n\n   micro avg       0.94      0.80      0.86      1198\n   macro avg       0.96      0.79      0.85      1198\nweighted avg       0.94      0.80      0.85      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.94      0.90      0.91       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.98      0.50      0.66       295\n\n   micro avg       0.94      0.80      0.87      1198\n   macro avg       0.95      0.79      0.85      1198\nweighted avg       0.95      0.80      0.86      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.94      0.89      0.91       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.97      0.51      0.67       295\n\n   micro avg       0.94      0.80      0.87      1198\n   macro avg       0.95      0.79      0.85      1198\nweighted avg       0.95      0.80      0.85      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [17/17 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n\n         LOC       0.94      0.89      0.91       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.97      0.51      0.67       295\n\n   micro avg       0.94      0.80      0.87      1198\n   macro avg       0.95      0.79      0.85      1198\nweighted avg       0.95      0.80      0.85      1198\n\" of type <class 'str'> for key \"eval/report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"name":"stdout","text":"Evaluation results for bert-base-multilingual-cased: {'eval_loss': 0.08399776369333267, 'eval_accuracy': 0.978070408325811, 'eval_report': '              precision    recall  f1-score   support\\n\\n         LOC       0.94      0.89      0.91       829\\n       PRICE       0.95      0.99      0.97        74\\n     Product       0.97      0.51      0.67       295\\n\\n   micro avg       0.94      0.80      0.87      1198\\n   macro avg       0.95      0.79      0.85      1198\\nweighted avg       0.95      0.80      0.85      1198\\n', 'eval_runtime': 2.1424, 'eval_samples_per_second': 125.091, 'eval_steps_per_second': 7.935, 'epoch': 15.0}\nModel: bert-base-multilingual-cased\nAccuracy: 0.978070408325811\n              precision    recall  f1-score   support\n\n         LOC       0.94      0.89      0.91       829\n       PRICE       0.95      0.99      0.97        74\n     Product       0.97      0.51      0.67       295\n\n   micro avg       0.94      0.80      0.87      1198\n   macro avg       0.95      0.79      0.85      1198\nweighted avg       0.95      0.80      0.85      1198\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
